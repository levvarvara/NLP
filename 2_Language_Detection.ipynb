{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "2_Language_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levvarvara/NLP/blob/master/2_Language_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgrrPZLc44t4",
        "colab_type": "text"
      },
      "source": [
        "# https://clck.ru/JC6Ea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jm3LqcY3Io4",
        "colab_type": "text"
      },
      "source": [
        "## Препроцессинг и определение языка / Preprocessing and language detection\n",
        "\n",
        "Предыдущая часть была посвящена предобработке текстовых данных, теперь применим эти методы на практике и заодно узнаем, как можно решать задачу определения языка.\n",
        "\n",
        "**Идеи?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFMHW4Ci3Io5",
        "colab_type": "text"
      },
      "source": [
        "Нам понадобятся некоторые пакеты для Питона, установите себе недостающие:\n",
        "* wikipedia\n",
        "* nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9kJxTkI3Io6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install wikipedia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urgPFXaE3Io8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzpLP1ve3Io-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter  # нужно объяснять, что это?\n",
        "\n",
        "import nltk\n",
        "import wikipedia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZM66tLa3IpA",
        "colab_type": "text"
      },
      "source": [
        "Возможно, перед первым запуском `nltk` нужно скачать немного данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCpai7Yc3IpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABmBn1Sz3IpC",
        "colab_type": "text"
      },
      "source": [
        "Теперь посмотрим, как можно скачивать тексты из Википедии с помощью пакета `wikipedia`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsBAih1d3IpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikipedia.set_lang('ru')\n",
        "page_name = wikipedia.random(1)\n",
        "page = wikipedia.page(page_name)\n",
        "print(page_name)\n",
        "print(page.title)\n",
        "print(page.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRQF5n93IpE",
        "colab_type": "text"
      },
      "source": [
        "Поскольку мы будем выбирать из нескольких языков, лучше сразу завести функцию, которая будет \n",
        "скачивать нужное количество страниц для заданного языка."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ0EfvHG3IpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_LANGS = ('kk', 'ru', 'uk', 'be', 'fr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA5fu5vo3IpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_texts_for_lang(lang, n=10):\n",
        "    wiki_content = []\n",
        "    ### ВАШ КОД ###\n",
        "    return wiki_content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a1XLXXb3IpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_texts = {}\n",
        "for lang in TEST_LANGS:\n",
        "    wiki_texts[lang] = get_texts_for_lang(lang, 50)\n",
        "    print(lang, len(wiki_texts[lang]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQmCT_J23IpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(wiki_texts['kk'][0])\n",
        "print(wiki_texts['fr'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOLiySe3IpN",
        "colab_type": "text"
      },
      "source": [
        "**Идея 1**\n",
        "\n",
        "Можно найти самые частотные слова для каждого языка, а потом считать их количество в каждом новом тексте."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SqFWksT3IpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_freqlist(wiki_pages, max_len=100):\n",
        "    freqlist = Counter()\n",
        "    ### ВАШ КОД ###\n",
        "    # не забудем про токенизацию - nltl.word_tokenize\n",
        "    return dict(freqlist.most_common(max_len))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrb4JNpk3IpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# проверка\n",
        "collect_freqlist(wiki_texts['fr'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVCHWvJ53IpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq_lists = {}\n",
        "for lang in TEST_LANGS:\n",
        "    freq_lists[lang] = collect_freqlist(wiki_texts[lang])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twrmWoH-3IpT",
        "colab_type": "text"
      },
      "source": [
        "Теперь всё готово, чтобы сделать первую определялку языка!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s303QEYg3IpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_lang_detect(freq_lists, text):\n",
        "    counts = Counter()\n",
        "    for lang, freq_list in freq_lists.items():\n",
        "        freq_list = Counter(freq_list)\n",
        "        for word in nltk.word_tokenize(text):\n",
        "            counts[lang] += int(freq_list[word] > 0)\n",
        "    return counts.most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3CtzkM73IpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_texts = get_texts_for_lang('fr')[0]\n",
        "print(test_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Z7VdXI3IpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simple_lang_detect(freq_lists, test_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnfJHW843IpY",
        "colab_type": "text"
      },
      "source": [
        "Попробуем оценить качество нашей определялки. Для этого установим `sklearn`, если его ещё нет, и возьмем оттуда функцию для подсчёта accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9kUURxH3IpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q25dDPmw3IpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ullc9-O3Ipb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_simple_lang_detect(freq_lists, test_size):\n",
        "    results = []  # сюда будем писать результаты\n",
        "    gold = []     # сюда будем писать исходный язык\n",
        "    for lang in TEST_LANGS:\n",
        "        ### ВАШ КОД ###\n",
        "    print(\"RESULTS:\")\n",
        "    print(\"%d languages\" % len(TEST_LANGS))\n",
        "    print(\"Test size: %d texts per language\" % test_size)\n",
        "    print(\"Accuracy: %.4f\" % accuracy_score(results, gold))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDavJybX3Ipd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# проверка\n",
        "test_simple_lang_detect(freq_lists, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiXZGRXM3Ipf",
        "colab_type": "text"
      },
      "source": [
        "## Машинное обучение\n",
        "\n",
        "Будем говорить о **supervised** методах (обучение с учителем):\n",
        "\n",
        "* У нас есть пары `(признаки, класс)`\n",
        "* Классификатор обучается на них — подбирает подходящую функцию отображения признаков в множество классов\n",
        "* После этого можно применять полученную модель для предсказаний на новых данных\n",
        "\n",
        "Признаки могут быть разные:\n",
        "* средняя длина слова\n",
        "* минимальная длина слова\n",
        "* максимальная длина слова\n",
        "* …"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWOfXYQb3Ipg",
        "colab_type": "text"
      },
      "source": [
        "Для определения языка часто используют не слова, а последовательности символов (символьные энграммы / character ngrams). Например, 3-граммы:\n",
        "```\n",
        "сим, имв, мво, вол, оло, лов\n",
        "```\n",
        "\n",
        "Из обучающих данных соберем словарь символьных энграмм $V$. \n",
        "Тогда каждый текст сможем представить в виде вектора длины $|V|$, где каждый признак показывает, присутствует ли соответствующая энграмма в тексте. Если эти значения будут просто $0$/$1$, то не учитывается \"важность\" последовательности для языка.\n",
        "\n",
        "Будем использовать $tf \\cdot idf$:\n",
        "\n",
        "$tf \\cdot idf (n, d) = \\frac{count(n_d)}{\\sum_{w \\in d}count(w)} \\cdot log\\frac{|D|}{|\\{d \\in D | n \\in d\\}|}$\n",
        "\n",
        "где $n$ - энграмма, $d$ - документ, а $D$ - весь корпус (на данном языке)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYCDVKsX3Iph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import feature_extraction\n",
        "\n",
        "vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
        "vectorizer.fit(wiki_texts['ru'])\n",
        "for item in vectorizer.get_feature_names()[:100]:\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WHMuiAF3Ipj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vectorizer.transform(wiki_texts['ru'])[0])  # первый документ в векторном представлении"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTmIMc9A3Ipl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install matplotlib==3.0.3\n",
        "#!pip install seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0ErpduK3Ipn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import pipeline\n",
        "from sklearn import naive_bayes\n",
        "import numpy as np\n",
        "\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ManAEc9n3Ipo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = pipeline.Pipeline([\n",
        "    ('vctr', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2), analyzer='char')),\n",
        "    ('clf', naive_bayes.MultinomialNB())\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6h0iZMI3Ipq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_texts = []\n",
        "lang_indices = []\n",
        "for lang in wiki_texts:\n",
        "    all_texts.extend(wiki_texts[lang])\n",
        "    lang_indices.extend([lang]*len(wiki_texts[lang]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvMe95v3Ips",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Обучаем классификатор\n",
        "clf.fit(np.array(all_texts), np.array(lang_indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk-znp6f3Ipt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Предсказываем результаты для тех же текстов\n",
        "clf.predict(all_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM-uxf993Ipv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Скачиваем новые тексты из вики и предсказываем еще раз\n",
        "clf.predict(get_texts_for_lang('be'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeoUXwXe3Ipw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Поделим корпус на train и test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_texts,\n",
        "                                                    lang_indices,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_predicted = clf.predict(X_test)\n",
        "cm = metrics.confusion_matrix(y_test, y_predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJDrn7gp3Ipy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_predicted)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQKO9WDx3Ipz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Нарисуем confusion matrix и оценим качество\n",
        "def test_classify(y_test, y_predicted, label_names):\n",
        "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "    \n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.ylim(10.5, -0.5)\n",
        "    sns.heatmap(cm, annot=True,  fmt='', xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    \n",
        "    print(metrics.classification_report(y_test, y_predicted,\n",
        "                                        target_names=label_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvUkK9oz3Ip0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_classify(y_test, y_predicted, clf.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VjSFWhX3Ip1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Проверим на случайных отрывках из тестовых текстов\n",
        "import random\n",
        "\n",
        "small_texts = []\n",
        "for text in X_test:\n",
        "    begin = random.randint(0, len(text) - 10)\n",
        "    small_texts.append(text[begin:begin+10])\n",
        "y_predicted_small = clf.predict(small_texts)\n",
        "test_classify(y_test, y_predicted_small, clf.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMuJP3Tp3Ip3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}